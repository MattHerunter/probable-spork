{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows importing Jupyter notebooks as modules\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import JupyterNotebookImporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM\n",
    "import keras.backend as kb\n",
    "import keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from keras import regularizers, optimizers\n",
    "from numpy.random import seed\n",
    "import AlgoPlotting as plt\n",
    "#from AlgoPlotting import XYChart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_it(X):\n",
    "    return np.expand_dims(X.reshape((-1,1)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randint(-5,5,200)\n",
    "\n",
    "n_data = len(data)\n",
    "data = np.matrix(data)\n",
    "n_train = int(0.8*n_data)\n",
    "\n",
    "index_delay = 5\n",
    "y_train = shape_it(data[:,:n_train])\n",
    "x_train = shape_it(data[:,index_delay:(n_train+index_delay)])\n",
    "y_test = shape_it(data[:,n_train:-index_delay])\n",
    "x_test = shape_it(data[:,(n_train+index_delay):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_chart = plt.XYChart(y=[x_train, y_train],\n",
    "                             names=['X Train','Y Train'],\n",
    "                             title='Training Data'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_chart = plt.XYChart(y=[x_test, y_test],\n",
    "                            names=['X Test','Y Test'],\n",
    "                            title='Testing Data'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDLearningRateTracker(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        opt = self.model.optimizer\n",
    "        \n",
    "        # Not sure why but eval'ing 1 by 1 then calculating doesn't crash, but 1 line does\n",
    "        lr = kb.eval(opt.lr)\n",
    "        decay = kb.eval(opt.decay)\n",
    "        iterations = kb.eval(opt.iterations)\n",
    "        #lr = kb.eval(opt.lr * (1. / (1. + opt.decay * opt.iterations))) # This crashes\n",
    "        lr = (lr * (1. / (1. + decay * iterations)))\n",
    "        #print('\\nLR: {:.6f}\\n'.format(lr))\n",
    "        logs['lr_calc'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_monitor(model, epochs, x_train, y_train, x_test, y_test, epoch_update_interval=10, losses_trim=0):\n",
    "    predictions = []\n",
    "    losses = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    predictions_chart = plt.XYChart(title='Predictions', x_label='Index', y_label='Y', names=['Prediction', 'Test Data'])\n",
    "    losses_chart = plt.XYChart(title='Losses', x_label='Epochs', y_label='Loss')\n",
    "    dlosses_chart = plt.XYChart(title='Diff(Losses)', x_label='Epochs', y_label='Diff(Loss)')\n",
    "    lr_chart = plt.XYChart(title='Learning Rate', x_label='Epochs', y_label='Learning Rate')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        history = model.fit(x_train, np.reshape(y_train,(-1,)), epochs=1, batch_size=batch_size, verbose=0, shuffle=False,\n",
    "                            callbacks=[SGDLearningRateTracker()])\n",
    "        loss = history.history['loss'][0]\n",
    "        lr = history.history['lr_calc'][0]\n",
    "        losses.append(loss)\n",
    "        learning_rates.append(lr)\n",
    "\n",
    "        #print('Epoch: ', epoch, ', Loss: ', loss)\n",
    "        model.reset_states()\n",
    "\n",
    "        # Update the plots\n",
    "        if not epoch % epoch_update_interval:\n",
    "            predictions = []\n",
    "            for ii in range(len(x_test)):\n",
    "                # make one-step forecast\n",
    "                X = x_test[ii]\n",
    "                X = X.reshape(1, 1, 1)\n",
    "                y_pred = model.predict(X, batch_size=batch_size)[0,0]\n",
    "\n",
    "                # store forecast\n",
    "                predictions.append(y_pred)\n",
    "                expected = y_test[ii]\n",
    "\n",
    "            # Push off the starting indices once the losses is long enough\n",
    "            if len(losses) <= losses_trim:\n",
    "                y_losses = losses\n",
    "                x_losses = np.arange(len(y_losses))\n",
    "            elif len(losses) > losses_trim and len(losses) < 2*losses_trim:\n",
    "                start = len(losses) - losses_trim\n",
    "                y_losses = losses[start:]\n",
    "                x_losses = np.arange(start, len(losses))\n",
    "            else:\n",
    "                y_losses = losses[losses_trim:]\n",
    "                x_losses = np.arange(losses_trim, len(losses))\n",
    "            \n",
    "            predictions_chart.update(y=[predictions, y_test])\n",
    "            losses_chart.update(x=x_losses, y=y_losses)\n",
    "            dlosses_chart.update(x=x_losses[:-1], y=np.diff(y_losses))\n",
    "            lr_chart.update(y=learning_rates)\n",
    "            model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "batch_size = 1\n",
    "los = 'mse'\n",
    "act = 'relu'\n",
    "learning_rate = 0.00005\n",
    "decay_rate = 0.00015 #learning_rate / epochs\n",
    "momentum = 0.0\n",
    "sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "opt = sgd\n",
    "\n",
    "model.add(SimpleRNN(12, batch_input_shape=(batch_size, x_train.shape[1], x_train.shape[2]),stateful=True, activation=act))\n",
    "model.add(Dense(12, activation=act))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.compile(loss=los, optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "epochs = 300\n",
    "epoch_update_interval = 5\n",
    "losses_trim = 20\n",
    "\n",
    "train_and_monitor(model, epochs, x_train, y_train, x_test, y_test,\n",
    "                  epoch_update_interval=epoch_update_interval, losses_trim=losses_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelR = Sequential()\n",
    "batch_size = 1\n",
    "los = 'mse'\n",
    "act = 'tanh'\n",
    "learning_rate = 0.00005\n",
    "decay_rate = 0.0001\n",
    "momentum = 0.0\n",
    "sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "opt = sgd\n",
    "kreg = regularizers.l2(0.01)\n",
    "areg = regularizers.l1(0.01)\n",
    "neurons = 30\n",
    "\n",
    "use_regularization = True\n",
    "\n",
    "if use_regularization:\n",
    "    modelR.add(SimpleRNN(neurons, batch_input_shape=(batch_size, x_train.shape[1], x_train.shape[2]), stateful=True,\n",
    "                         kernel_regularizer=kreg, activity_regularizer=areg))\n",
    "    modelR.add(Dense(neurons, kernel_regularizer=kreg, activity_regularizer=areg, activation=act))\n",
    "    modelR.add(Dense(1,kernel_regularizer=kreg, activity_regularizer=areg))\n",
    "    modelR.compile(loss=los, optimizer=opt, metrics=['accuracy'])\n",
    "else:\n",
    "    modelR.add(SimpleRNN(neurons, batch_input_shape=(batch_size, x_train.shape[1], x_train.shape[2]), stateful=True))\n",
    "    modelR.add(Dense(neurons, activation=act))\n",
    "    modelR.add(Dense(1))\n",
    "    modelR.compile(loss=los, optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "epochs = 500\n",
    "epoch_update_interval = 5\n",
    "losses_trim = 100\n",
    "\n",
    "train_and_monitor(modelR, epochs, x_train, y_train, x_test, y_test,\n",
    "                  epoch_update_interval=epoch_update_interval, losses_trim=losses_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelR = Sequential()\n",
    "batch_size = 1\n",
    "los = 'mse'\n",
    "act = 'tanh'\n",
    "learning_rate = 0.00005\n",
    "decay_rate = 0.0001\n",
    "momentum = 0.0\n",
    "sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "opt = sgd\n",
    "kreg = regularizers.l2(0.01)\n",
    "areg = regularizers.l1(0.01)\n",
    "neurons = 10\n",
    "\n",
    "use_regularization = False\n",
    "\n",
    "if use_regularization:\n",
    "    modelR.add(LSTM(neurons, batch_input_shape=(batch_size, x_train.shape[1], x_train.shape[2]), stateful=True,\n",
    "                         kernel_regularizer=kreg, activity_regularizer=areg))\n",
    "    modelR.add(Dense(neurons, kernel_regularizer=kreg, activity_regularizer=areg, activation=act))\n",
    "    modelR.add(Dense(1,kernel_regularizer=kreg, activity_regularizer=areg))\n",
    "    modelR.compile(loss=los, optimizer=opt, metrics=['accuracy'])\n",
    "else:\n",
    "    modelR.add(SimpleRNN(neurons, batch_input_shape=(batch_size, x_train.shape[1], x_train.shape[2]), stateful=True))\n",
    "    modelR.add(Dense(neurons, activation=act))\n",
    "    modelR.add(Dense(1))\n",
    "    modelR.compile(loss=los, optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "epochs = 2000\n",
    "epoch_update_interval = 5\n",
    "losses_trim = 100\n",
    "\n",
    "train_and_monitor(modelR, epochs, x_train, y_train, x_test, y_test,\n",
    "                  epoch_update_interval=epoch_update_interval, losses_trim=losses_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One step forecast on testing data\n",
    "model.reset_states()\n",
    "model.predict(X_train, batch_size=batch_size)\n",
    "predictions = []\n",
    "for i in range(len(X_test)):\n",
    "    # make one-step forecast\n",
    "    X = X_test[i]\n",
    "    X = X.reshape(1, 1, 1)\n",
    "    yhat = model.predict(X, batch_size=batch_size)[0,0]\n",
    "\n",
    "    # store forecast\n",
    "    predictions.append(yhat)\n",
    "    expected = Y_test[ i ]\n",
    "    print('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "# report performance\n",
    "rmse = np.sqrt(mean_squared_error(Y_test.reshape(len(Y_test)), predictions))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic forecast on test data\n",
    "model.reset_states()\n",
    "model.predict(X_train, batch_size=batch_size)\n",
    "\n",
    "dynpredictions = list()\n",
    "dyhat = X_test[0]\n",
    "\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    # make one-step forecast\n",
    "    dyhat = yhat.reshape(1, 1, 1)\n",
    "    dyhat = model.predict(dyhat, batch_size=batch_size)[0,0]\n",
    "\n",
    "    # store forecast\n",
    "    dynpredictions.append(dyhat)\n",
    "    expected = Y_test[ i ]\n",
    "    print('Month=%d, Predicted Dynamically=%f, Expected=%f' % (i+1, dyhat, expected))\n",
    "\n",
    "\n",
    "drmse = np.sqrt(mean_squared_error(Y_test.reshape(len(Y_test)), dynpredictions))\n",
    "print('Test Dynamic RMSE: %.3f' % drmse)\n",
    "# line plot of observed vs predicted\n",
    "plt.plot(Y_test.reshape(len(Y_test)))\n",
    "plt.plot(dynpredictions)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
