{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows importing Jupyter notebooks as modules\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import JupyterNotebookImporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as kb\n",
    "import keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from keras import regularizers, optimizers, initializers\n",
    "from numpy.random import seed\n",
    "import AlgoPlotting as plt\n",
    "import AlgoPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_it(X):\n",
    "    return np.expand_dims(X.reshape((-1,1)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate data\n",
    "# data = np.random.randint(-5,5,200)\n",
    "\n",
    "# n_data = len(data)\n",
    "# data = np.matrix(data)\n",
    "# n_train = int(0.8*n_data)\n",
    "\n",
    "# index_delay = 5\n",
    "# y_train = shape_it(data[:,:n_train])\n",
    "# x_train = shape_it(data[:,index_delay:(n_train+index_delay)])\n",
    "# y_test = shape_it(data[:,n_train:-index_delay])\n",
    "# x_test = shape_it(data[:,(n_train+index_delay):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    \n",
    "    N = 200\n",
    "    delay = 10\n",
    "    #data = np.random.randint(-1, 2, N + delay)\n",
    "    data = (np.random.rand(N + delay))\n",
    "    data2 = (np.random.rand(N + delay))\n",
    "    # data = np.cumsum(data)\n",
    "    x = data[delay:]\n",
    "    x2 = data2[delay:]\n",
    "    #y = [data[ii]+data[ii+1]**2 + data[ii+2]**0.5 for ii in range(N)]\n",
    "    y = [(data[ii]+data[ii+1])*x2[ii] for ii in range(N)]\n",
    "    \n",
    "#     x = []\n",
    "#     for ii in range(n):\n",
    "#         if np.random.rand() > 0.01 + 0.99 * ii/n:\n",
    "#             x.append(-1)\n",
    "#         else:\n",
    "#             x.append(1)\n",
    "    \n",
    "#     y = x[1:]\n",
    "#     x = x[0:-1]\n",
    "        \n",
    "#     print(x)\n",
    "#     print(y)\n",
    "#     print(np.shape(x))\n",
    "#     print(np.shape(y))\n",
    "    \n",
    "    x = shape_it(np.array(x))\n",
    "    x2 = shape_it(np.array(x2))\n",
    "    #print(x.shape)\n",
    "    x = np.hstack((x, x2))\n",
    "    #print(x.shape)\n",
    "    y = shape_it(np.array(y))\n",
    "    #print(y.shape)\n",
    "    return x, y\n",
    "    \n",
    "#generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "# training_chart = plt.XYChart(y=[x_train, y_train],\n",
    "#                              names=['X Train','Y Train'],\n",
    "#                              title='Training Data'\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot testing data\n",
    "# testing_chart = plt.XYChart(y=[x_test, y_test],\n",
    "#                             names=['X Test','Y Test'],\n",
    "#                             title='Testing Data'\n",
    "#                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDLearningRateTracker(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        opt = self.model.optimizer\n",
    "        \n",
    "        # Not sure why but eval'ing 1 by 1 then calculating doesn't crash, but 1 line does\n",
    "        lr = kb.eval(opt.lr)\n",
    "        decay = kb.eval(opt.decay)\n",
    "        iterations = kb.eval(opt.iterations)\n",
    "        #lr = kb.eval(opt.lr * (1. / (1. + opt.decay * opt.iterations))) # This crashes\n",
    "        lr = (lr * (1. / (1. + decay * iterations)))\n",
    "        #print('\\nLR: {:.6f}\\n'.format(lr))\n",
    "        logs['lr_calc'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternPredictingNetwork:\n",
    "    def __init__(self, model_dict, x_train, y_train, x_test, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.model = Sequential()\n",
    "        self.batch_size = 1\n",
    "        \n",
    "        batch_input_shape=(self.batch_size, self.x_train.shape[1], self.x_train.shape[2])\n",
    "        use_rnn_model = True\n",
    "        \n",
    "        if use_rnn_model:\n",
    "            los = 'mse'\n",
    "            act = 'tanh'\n",
    "            learning_rate = 0.0001*9\n",
    "            decay_rate = learning_rate/100\n",
    "            momentum = 0.0\n",
    "            sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "            opt = sgd\n",
    "            kreg = regularizers.l2(0.001)\n",
    "            areg = regularizers.l1(0.001)\n",
    "            neurons = 30\n",
    "            num_recurrent_layers = 3\n",
    "            for _ in range(max(num_recurrent_layers - 1, 0)):\n",
    "                self.model.add(SimpleRNN(neurons, activation=act, batch_input_shape=batch_input_shape, stateful=True, return_sequences=True,\n",
    "                                         kernel_regularizer=kreg, activity_regularizer=areg))\n",
    "            self.model.add(SimpleRNN(neurons, activation=act, batch_input_shape=batch_input_shape, stateful=True,\n",
    "                                     kernel_regularizer=kreg, activity_regularizer=areg))\n",
    "            self.model.add(Dense(neurons, activation=act, \n",
    "                                 kernel_regularizer=kreg, activity_regularizer=areg))\n",
    "            self.model.add(Dense(1))\n",
    "\n",
    "            self.model.compile(loss=los, optimizer=opt)\n",
    "        else:\n",
    "            los = 'mse'\n",
    "            act = 'tanh'\n",
    "            learning_rate = 0.01*9\n",
    "            decay_rate = learning_rate/100\n",
    "            momentum = 0.0\n",
    "            sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "            opt = sgd\n",
    "            kreg = regularizers.l2(0.0001)\n",
    "            areg = regularizers.l2(0.0001)\n",
    "            neurons = 30\n",
    "            num_recurrent_layers = 2\n",
    "            for _ in range(max(num_recurrent_layers - 1, 0)):\n",
    "                self.model.add(LSTM(neurons, activation=act, batch_input_shape=batch_input_shape, stateful=True, return_sequences=True,\n",
    "                                    kernel_regularizer=kreg, activity_regularizer=areg))\n",
    "            self.model.add(LSTM(neurons, activation=act, batch_input_shape=batch_input_shape, stateful=True,\n",
    "                                kernel_regularizer=kreg, activity_regularizer=areg))\n",
    "            self.model.add(Dense(neurons, activation=act, \n",
    "                                 kernel_regularizer=kreg, activity_regularizer=areg))\n",
    "            self.model.add(Dense(1))\n",
    "\n",
    "            self.model.compile(loss=los, optimizer=opt)\n",
    "        \n",
    "        \n",
    "    def train_and_monitor(self, epochs=100, epoch_update_interval=10, losses_trim=0, losses_window=None):\n",
    "        # Initialize empty lists\n",
    "        predictions = []\n",
    "        losses = []\n",
    "        learning_rates = []\n",
    "        \n",
    "        # Initialize charts\n",
    "        predictions_chart = plt.XYChart(title='Predictions', x_label='Index', y_label='Y', names=['Prediction', 'Test Data'])\n",
    "        losses_chart = plt.XYChart(title='Losses', x_label='Epochs', y_label='Loss')\n",
    "        dlosses_chart = plt.XYChart(title='Diff(Losses)', x_label='Epochs', y_label='Diff(Loss)')\n",
    "        lr_chart = plt.XYChart(title='Learning Rate', x_label='Epochs', y_label='Learning Rate')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.x_train, self.y_train = generate_data()\n",
    "            self.x_test, self.y_test = generate_data()\n",
    "            #self.x_test = self.x_train\n",
    "            #self.y_test = self.y_train\n",
    "            \n",
    "            #self.model.predict(shape_it(np.array(-1)), batch_size=batch_size)\n",
    "            history = self.model.fit(self.x_train, np.reshape(self.y_train,(-1,)), epochs=1,\n",
    "                                     batch_size=self.batch_size, verbose=0, shuffle=False, callbacks=[SGDLearningRateTracker()])\n",
    "#             history = self.model.fit(self.x_train, self.y_train, epochs=1,\n",
    "#                                      batch_size=batch_size, verbose=0, shuffle=False, callbacks=[SGDLearningRateTracker()])\n",
    "            loss = history.history['loss'][0]\n",
    "            lr = history.history['lr_calc'][0]\n",
    "            losses.append(loss)\n",
    "            learning_rates.append(lr)\n",
    "\n",
    "            # Update the plots\n",
    "            if not epoch % epoch_update_interval:\n",
    "                self.model.reset_states()\n",
    "                predictions = []\n",
    "                predictions = self.model.predict(self.x_test, batch_size=self.batch_size, verbose=0)\n",
    "                expected = self.y_test\n",
    "                \n",
    "                # Push off the starting indices once the losses is long enough\n",
    "                if losses_window is not None:\n",
    "                    if len(losses) <= losses_window:\n",
    "                            y_losses = losses\n",
    "                            x_losses = np.arange(len(y_losses))\n",
    "                    else:\n",
    "                        start = len(losses) - losses_window\n",
    "                        y_losses = losses[start:]\n",
    "                        x_losses = np.arange(start, len(losses))\n",
    "                else:\n",
    "                    if len(losses) <= losses_trim:\n",
    "                        y_losses = losses\n",
    "                        x_losses = np.arange(len(y_losses))\n",
    "                    elif len(losses) > losses_trim and len(losses) < 2*losses_trim:\n",
    "                        start = len(losses) - losses_trim\n",
    "                        y_losses = losses[start:]\n",
    "                        x_losses = np.arange(start, len(losses))\n",
    "                    else:\n",
    "                        y_losses = losses[losses_trim:]\n",
    "                        x_losses = np.arange(losses_trim, len(losses))\n",
    "\n",
    "                predictions_chart.update(y=[predictions, self.y_test])\n",
    "                losses_chart.update(x=x_losses, y=y_losses)\n",
    "                dlosses_chart.update(x=x_losses[:-1], y=np.diff(y_losses))\n",
    "                lr_chart.update(y=learning_rates)\n",
    "            self.model.reset_states()\n",
    "    \n",
    "    # Forecast with updates each step\n",
    "    def one_step_forecast(self):\n",
    "        batch_size = 1\n",
    "        # One step forecast on testing data\n",
    "        self.model.reset_states()\n",
    "        self.model.predict(self.x_train, batch_size=batch_size)\n",
    "        predictions = []\n",
    "        for ii in range(len(self.x_test)):\n",
    "            # make one-step forecast\n",
    "            X = self.x_test[ii]\n",
    "            X = X.reshape(1, 1, 1)\n",
    "            yhat = self.model.predict(X, batch_size=batch_size)[0,0]\n",
    "\n",
    "            # store forecast\n",
    "            predictions.append(yhat)\n",
    "            expected = self.y_test[ii]\n",
    "            print('Index=%d, Predicted=%f, Expected=%f' % (ii, yhat, expected))\n",
    "\n",
    "        # report performance\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_test.reshape(len(self.y_test)), predictions))\n",
    "        print('Test RMSE: %.3f' % rmse)\n",
    "        xy_chart = plt.XYChart(y=[predictions, self.y_test], names=['Prediction', 'Test Data'], title='One Step Prediction')\n",
    "        \n",
    "    def dynamic_forecast(self):\n",
    "        batch_size = 1\n",
    "        # Dynamic forecast on test data\n",
    "        self.model.reset_states()\n",
    "        self.model.predict(self.x_train, batch_size=batch_size)\n",
    "\n",
    "        dynpredictions = list()\n",
    "        dyhat = self.x_test[0]\n",
    "\n",
    "\n",
    "        for ii in range(len(self.x_test)):\n",
    "            # make one-step forecast\n",
    "            dyhat = yhat.reshape(1, 1, 1)\n",
    "            dyhat = model.predict(dyhat, batch_size=batch_size)[0,0]\n",
    "\n",
    "            # store forecast\n",
    "            dynpredictions.append(dyhat)\n",
    "            expected = self.y_test[ii]\n",
    "            print('Index=%d, Predicted Dynamically=%f, Expected=%f' % (ii, dyhat, expected))\n",
    "\n",
    "\n",
    "        drmse = np.sqrt(mean_squared_error(Y_test.reshape(len(self.y_test)), dynpredictions))\n",
    "        print('Test Dynamic RMSE: %.3f' % drmse)\n",
    "        xy_chart = plt.XYChart(y=[dynpredictions, self.y_test], names=['Dynamic Prediction', 'Test Data'], title='Dynamic Prediction')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "epochs = 1000\n",
    "epoch_update_interval = 10\n",
    "x_train, y_train = generate_data()\n",
    "x_test, y_test = generate_data()\n",
    "pattern_predicting_network = PatternPredictingNetwork(model_dict, x_train, y_train, x_test, y_test)\n",
    "# pattern_predicting_network.model.layers[0].set_weights(\n",
    "#     #[np.array([[1.048831]]), np.array([[-0.05480201]])]\n",
    "#     [np.array([[0.95]]), np.array([[0]])]\n",
    "# )\n",
    "# pattern_predicting_network.model.reset_states()\n",
    "# pattern_predicting_network.model.predict(shape_it(np.array(-1)), batch_size=1)\n",
    "pattern_predicting_network.train_and_monitor(epochs=epochs, epoch_update_interval=epoch_update_interval, losses_window=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pattern_predicting_network.one_step_forecast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LossLearningRateScheduler(keras.callbacks.History):\n",
    "    \"\"\"\n",
    "    A learning rate scheduler that relies on changes in loss function\n",
    "    value to dictate whether learning rate is decayed or not.\n",
    "    LossLearningRateScheduler has the following properties:\n",
    "    base_lr: the starting learning rate\n",
    "    lookback_epochs: the number of epochs in the past to compare with the loss function at the current epoch to determine if progress is being made.\n",
    "    decay_threshold / decay_multiple: if loss function has not improved by a factor of decay_threshold * lookback_epochs, then decay_multiple will be applied to the learning rate.\n",
    "    spike_epochs: list of the epoch numbers where you want to spike the learning rate.\n",
    "    spike_multiple: the multiple applied to the current learning rate for a spike.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr, lookback_epochs, spike_epochs = None, spike_multiple = 10, decay_threshold = 0.002, decay_multiple = 0.5, loss_type = 'val_loss'):\n",
    "\n",
    "        super(LossLearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.lookback_epochs = lookback_epochs\n",
    "        self.spike_epochs = spike_epochs\n",
    "        self.spike_multiple = spike_multiple\n",
    "        self.decay_threshold = decay_threshold\n",
    "        self.decay_multiple = decay_multiple\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \n",
    "        if len(self.epoch) > self.lookback_epochs:\n",
    "\n",
    "            current_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "            target_loss = self.history[self.loss_type] \n",
    "\n",
    "            loss_diff =  target_loss[-int(self.lookback_epochs)] - target_loss[-1]\n",
    "\n",
    "            if loss_diff <= np.abs(target_loss[-1]) * (self.decay_threshold * self.lookback_epochs):\n",
    "\n",
    "                print(' '.join(('Changing learning rate from', str(current_lr), 'to', str(current_lr * self.decay_multiple))))\n",
    "                K.set_value(self.model.optimizer.lr, current_lr * self.decay_multiple)\n",
    "                current_lr = current_lr * self.decay_multiple\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(' '.join(('Learning rate:', str(current_lr))))\n",
    "\n",
    "            if self.spike_epochs is not None and len(self.epoch) in self.spike_epochs:\n",
    "                print(' '.join(('Spiking learning rate from', str(current_lr), 'to', str(current_lr * self.spike_multiple))))\n",
    "                K.set_value(self.model.optimizer.lr, current_lr * self.spike_multiple)\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(' '.join(('Setting learning rate to', str(self.base_lr))))\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "\n",
    "\n",
    "        return K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
