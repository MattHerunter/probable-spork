{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Experiment \\#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "import ipywidgets\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import bisect\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMarket:\n",
    "    def __init__(self):\n",
    "        self.curr_index = np.floor(np.random.rand()*10)\n",
    "        self.position = False\n",
    "        self.bought_price = None\n",
    "        \n",
    "    def get_price(self, index):\n",
    "        period = 10\n",
    "        ii = index % period\n",
    "        #return ii\n",
    "        if ii < period/2:\n",
    "            return ii\n",
    "        else:\n",
    "            return period-ii\n",
    "        \n",
    "    def buy(self):\n",
    "        if not self.position:\n",
    "            self.position = True\n",
    "            self.bought_price = self.get_price(self.curr_index)\n",
    "        else:\n",
    "            error('FUCK out of shares')\n",
    "            \n",
    "    def sell(self):\n",
    "        if self.position:\n",
    "            self.position = False\n",
    "            reward = self.get_price(self.curr_index) - self.bought_price\n",
    "            self.bought_price = None\n",
    "            return reward\n",
    "        else:\n",
    "            error('FUCK out of shares')\n",
    "            \n",
    "    def tick(self):\n",
    "        self.curr_index += 1\n",
    "        return self.get_price(self.curr_index)-self.get_price(self.curr_index-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from operator import add\n",
    "\n",
    "\n",
    "class DQNAgent(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.inputs = 2\n",
    "        self.outputs = 2\n",
    "        self.model = self.network()\n",
    "        self.memory = []\n",
    "        self.short_memory = []\n",
    "        self.gamma = 1 \n",
    "        \n",
    "    def get_state(self, market):\n",
    "        state = np.diff([market.get_price(market.curr_index-self.inputs+ii+1) for ii in range(self.inputs)])\n",
    "        state = [max(x,0) for x in state]\n",
    "        state = [1 if market.position else 0] + state\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def network(self, weights=None):\n",
    "        model = Sequential()\n",
    "        \n",
    "        num_layers = 1\n",
    "        neurons_per_layer = 3\n",
    "        dropout = 0.0\n",
    "        reg = 0.00\n",
    "        act = 'tanh'\n",
    "        output_act = 'softmax'\n",
    "        learning = 0.005\n",
    "        opt = optimizers.Adam(learning)\n",
    "        \n",
    "        if num_layers == 0:\n",
    "            if dropout > 0:\n",
    "                    model.add(Dropout(dropout))\n",
    "            model.add(Dense(units=self.outputs, activation=output_act, input_dim=self.inputs,\n",
    "                        kernel_regularizer=regularizers.l1(reg),\n",
    "                        activity_regularizer=regularizers.l1(reg)))\n",
    "        else:\n",
    "            model.add(Dense(units=neurons_per_layer, activation=act, input_dim=self.inputs,\n",
    "                            kernel_regularizer=regularizers.l1(reg),\n",
    "                            activity_regularizer=regularizers.l1(reg)))\n",
    "            \n",
    "            for ii in range(num_layers - 1):\n",
    "                if dropout > 0:\n",
    "                    model.add(Dropout(dropout))\n",
    "                model.add(Dense(units=neurons_per_layer, activation=act,\n",
    "                                kernel_regularizer=regularizers.l1(reg),\n",
    "                                activity_regularizer=regularizers.l1(reg)))\n",
    "                \n",
    "            if dropout > 0:\n",
    "                    model.add(Dropout(dropout))\n",
    "            model.add(Dense(units=self.outputs, activation=output_act,\n",
    "                            kernel_regularizer=regularizers.l1(reg),\n",
    "                            activity_regularizer=regularizers.l1(reg))) \n",
    "    \n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "        self.short_memory.append((state, action, reward, next_state))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        if len(self.short_memory) > 1000:\n",
    "            minibatch_short = random.sample(self.short_memory, 1000)\n",
    "        else:\n",
    "            minibatch_short = self.short_memory\n",
    "        if len(self.memory) > 1000:\n",
    "            minibatch_long = random.sample(self.memory, 1000)\n",
    "        else:\n",
    "            minibatch_long = self.memory\n",
    "            \n",
    "        minibatch = minibatch_short + minibatch_long\n",
    "            \n",
    "        state_array = len(minibatch) * [None]\n",
    "        target_f_array = len(minibatch) * [None]   \n",
    "        index = 0\n",
    "        for state, action, reward, next_state in minibatch:\n",
    "            target = reward + self.gamma * np.amax(self.model.predict(np.array([next_state]))[0])\n",
    "            target_f = self.model.predict(np.array([state]))\n",
    "            target_f[0][np.argmax(action)] = target\n",
    "            \n",
    "            state_array[index] = np.asarray(state)\n",
    "            target_f_array[index] = target_f[0]\n",
    "            \n",
    "            #print('State ', state, ', Action ', action)\n",
    "            #print('Reward ', reward, ', Target ', target, ', Targetf ', target_f)\n",
    "            index += 1\n",
    "        state_array = np.asarray(state_array)\n",
    "        target_f_array = np.asarray(target_f_array)\n",
    "        self.model.fit(state_array, target_f_array, epochs=1, verbose=0, batch_size = 22)\n",
    "        self.short_memory = []\n",
    "        \n",
    "    def train_single(self, state, action, reward, next_state):\n",
    "        target = reward + self.gamma * np.amax(self.model.predict(np.array([next_state]))[0])\n",
    "        print('target', target)\n",
    "        target_f = self.model.predict(np.array([state]))\n",
    "        print('target_f', target_f)\n",
    "        target_f[0][np.argmax(action)] = target\n",
    "        print('target_f', target_f)\n",
    "        \n",
    "#         print(\"wow\")\n",
    "#         print(\"Output before fit: \", self.model.predict(np.array([state])))\n",
    "        prev_loss = self.model.evaluate(np.array([state]), target_f, verbose=0)\n",
    "#         print(\"Target output: \", target_f[0])\n",
    "        self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n",
    "#         print(\"Output after fit: \", self.model.predict(np.array([state])))\n",
    "#         print(\"Change in loss function: \", self.model.evaluate(np.array([state]), target_f, verbose=0) - prev_loss)\n",
    "        \n",
    "    def predict(self,state):\n",
    "        return self.model.predict(np.asarray(state).reshape(1, self.inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chart:\n",
    "    def __init__(self):\n",
    "        data1 = [dict(\n",
    "                type = 'heatmap',\n",
    "                x = [],\n",
    "                y = [],\n",
    "                z = [],\n",
    "                colorscale = [[0,'black'],[0.5, 'rgb(182, 218, 249)'],[0.5,'rgb(249, 183, 182)'],[1,'black']]),\n",
    "                 \n",
    "                dict(type='scatter',\n",
    "                    x = [0,0,1,1,0],\n",
    "                    y = [0,1,1,0,0],\n",
    "                    mode='lines',\n",
    "                    line=dict(\n",
    "                        color='black',\n",
    "                        width=1\n",
    "                    ))]\n",
    "        data2 = [dict(\n",
    "                type = 'heatmap',\n",
    "                x = [],\n",
    "                y = [],\n",
    "                z = [])]\n",
    "        data3 = [dict(\n",
    "                type = 'heatmap',\n",
    "                x = [],\n",
    "                y = [],\n",
    "                z = [])]\n",
    "        \n",
    "        layout = dict(\n",
    "                title = 'Plot',\n",
    "                width = 300,\n",
    "                height = 300)\n",
    "        \n",
    "        self.chart1 = go.FigureWidget( data=data1, layout=layout)\n",
    "        self.chart2 = go.FigureWidget( data=data2, layout=layout)\n",
    "        self.chart3 = go.FigureWidget( data=data3, layout=layout)\n",
    "        \n",
    "        self.display = ipywidgets.HBox([self.chart1, self.chart2, self.chart3])\n",
    "        \n",
    "chart = Chart()\n",
    "chart.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from random import randint\n",
    "\n",
    "def run(chart):\n",
    "    agent = DQNAgent()\n",
    "#     agent.model.layers[0].set_weights(\n",
    "#         [np.array([[-0.27110755, -1.0356677 ,  0.03980407],\n",
    "#        [-0.760168  ,  0.50631946, -0.1097751 ]]), np.array([ 0.00288957, -0.00263686,  0.0055096 ])]\n",
    "#     )\n",
    "#     agent.model.layers[1].set_weights(\n",
    "#         [np.array([[-0.89726514,  0.35921633],\n",
    "#        [ 1.0436928 , -0.6169437 ],\n",
    "#        [-0.04465797,  0.18045507]]), np.array([0.00031996, 0.10000])]\n",
    "#     )\n",
    "    \n",
    "    num_tests = 1\n",
    "    ticks_per_test = 100\n",
    "    \n",
    "    total_reward_old = -50\n",
    "    \n",
    "    for ii in range(num_tests):\n",
    "        market = ModelMarket()\n",
    "        epsilon = 0#(10)*(total_reward_old < 30)#200 - 10 * ii #randomness\n",
    "        \n",
    "        total_reward = 0\n",
    "        for jj in range(ticks_per_test):\n",
    "            state_old = agent.get_state(market)\n",
    "            \n",
    "            #perform random actions based on agent.epsilon, or choose the action\n",
    "            if randint(0, 200) < epsilon and False:\n",
    "                action = to_categorical(randint(0, 1), num_classes=2)\n",
    "            else:\n",
    "                # predict action based on the old state\n",
    "                prediction = agent.predict(state_old)\n",
    "                action = to_categorical(np.argmax(prediction[0]), num_classes=2)\n",
    "            \n",
    "            reward = 0\n",
    "\n",
    "#             # 1 means flip position, 0 means hold\n",
    "#             if np.argmax(action) == 1:\n",
    "#                 if not market.position:\n",
    "#                     market.buy()\n",
    "#                 else:\n",
    "#                     reward = market.sell()\n",
    "            \n",
    "            # 0 means buy/hold, 0 means sell or stay out\n",
    "            if np.argmax(action) == 0:\n",
    "                if not market.position:\n",
    "                    market.buy()\n",
    "            if np.argmax(action) == 1:\n",
    "                if market.position:\n",
    "                    reward = market.sell()\n",
    "            market.tick()\n",
    "                    \n",
    "#             if np.argmax(action) == 0:\n",
    "#                 if not market.position:\n",
    "#                     market.buy()\n",
    "#                 reward = market.tick()\n",
    "#             else:\n",
    "#                 if market.position:\n",
    "#                     market.sell()\n",
    "#                 reward = -market.tick()\n",
    "                \n",
    "            state_new = agent.get_state(market)\n",
    "            \n",
    "            total_reward += reward\n",
    "            agent.remember(state_old, action, reward, state_new)\n",
    "            \n",
    "#             print(\"wow\")\n",
    "#             print(state_old)\n",
    "#             print(action)\n",
    "#             print(reward)\n",
    "            agent.train_single(state_old, action, reward, state_new)\n",
    "        \n",
    "#         for layer in agent.model.layers:\n",
    "#             print(layer.get_weights())\n",
    "        \n",
    "        agent.train()\n",
    "        print('Game', ii, '      Score:', total_reward, '      Epsilon:', epsilon)\n",
    "        total_reward_old = total_reward\n",
    "        \n",
    "        batch = []\n",
    "        #step = 0.02\n",
    "        steps = 100\n",
    "        xmin = -0.5\n",
    "        xmax = 1.5\n",
    "        ymin = -0.5\n",
    "        ymax = 1.5\n",
    "        vec = np.linspace(xmin, xmax, steps)\n",
    "        #for x in np.arange(0.0, 1.0, step):\n",
    "        for x in vec:\n",
    "            #for y in np.arange(0.0, 1.0, step):\n",
    "            for y in vec:\n",
    "                batch.append(np.asarray([y, x]))\n",
    "        q = agent.model.predict_on_batch(np.asarray(batch))\n",
    "        X,Y = np.meshgrid(vec,vec)\n",
    "        \n",
    "        z1 = []\n",
    "        z2 = []\n",
    "        for ii in range(steps):\n",
    "            Z1 = []\n",
    "            Z2 = []\n",
    "            for p in q[steps*(ii):steps*(ii+1)]:\n",
    "                Z1.append(p[0])\n",
    "                Z2.append(p[1])\n",
    "            z1.append(Z1)\n",
    "            z2.append(Z2)\n",
    "           \n",
    "        X = np.reshape(X,(1,steps**2))[0]\n",
    "        Y = np.reshape(Y,(1,steps**2))[0]\n",
    "        z1 = np.reshape(z1,(1,steps**2))[0]\n",
    "        z2 = np.reshape(z2,(1,steps**2))[0]\n",
    "        z = z1-z2\n",
    "        \n",
    "        \n",
    "        chart.chart1.data[0].x = X\n",
    "        chart.chart1.data[0].y = Y\n",
    "        chart.chart1.data[0].z = z\n",
    "        chart.chart1.data[0].zmax = np.max(np.abs(z))\n",
    "        chart.chart1.data[0].zmin = -np.max(np.abs(z))\n",
    "        \n",
    "        chart.chart2.data[0].x = X\n",
    "        chart.chart2.data[0].y = Y\n",
    "        chart.chart2.data[0].z = z1\n",
    "        \n",
    "        chart.chart3.data[0].x = X\n",
    "        chart.chart3.data[0].y = Y\n",
    "        chart.chart3.data[0].z = z2\n",
    "\n",
    "        \n",
    "run(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
